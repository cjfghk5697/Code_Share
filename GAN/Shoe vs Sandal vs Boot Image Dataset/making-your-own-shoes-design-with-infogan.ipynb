{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# InfoGAN\n[InfoGAN](https://arxiv.org/abs/1606.03657) is an information-theoretic extension to the simple Generative Adversarial Networks that is able to learn disentangled representations in a completely unsupervised manner. What this means is that InfoGAN successfully disentangle wrirting styles from digit shapes on th MNIST dataset and discover visual concepts such as hair styles and gender on the CelebA dataset. To achieve this an information-theoretic regularization is added to the loss function that enforces the maximization of mutual information between latent codes, c, and the generator distribution G(z, c).\n\n**You can see more detail in My [Github](https://github.com/cjfghk5697/Paper_Implementation/tree/main/InfoGAN) repositories. And SVHN dataset was implementation. dataset **\n","metadata":{"id":"W0RyOtzFmTnh"}},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"-o_VnNdraKcI"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torchvision.utils as vutils\nimport time\nimport torch.optim as optim\nfrom PIL import Image\nfrom torch.utils.data import Dataset,DataLoader","metadata":{"id":"ND-tE4kGtdS7","execution":{"iopub.status.busy":"2022-11-07T10:25:18.962552Z","iopub.execute_input":"2022-11-07T10:25:18.963045Z","iopub.status.idle":"2022-11-07T10:25:18.977227Z","shell.execute_reply.started":"2022-11-07T10:25:18.963004Z","shell.execute_reply":"2022-11-07T10:25:18.974856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"params={\n    'epochs':100,\n    'image_size':64,\n    'batch_size':256,\n    'num_workers':2,\n    'nz':74,\n    'nc':3,\n    'ngf':64,\n    'ndf':64,\n    'lr':1e-3,\n    'beta1':0.5\n}\n","metadata":{"id":"OUrk9raQwust","execution":{"iopub.status.busy":"2022-11-07T10:25:18.979624Z","iopub.execute_input":"2022-11-07T10:25:18.980693Z","iopub.status.idle":"2022-11-07T10:25:18.992727Z","shell.execute_reply.started":"2022-11-07T10:25:18.980647Z","shell.execute_reply":"2022-11-07T10:25:18.991482Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Data\nThe dataset have Sandal, Shoe, Boot classes. We try to the generate image about 3 classes.","metadata":{"id":"qn3WlR76aNnw"}},{"cell_type":"code","source":"states=['Boot','Sandal','Shoe']\nimage_files=[[os.path.join('/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset',state,x) for x in os.listdir(os.path.join('/kaggle/input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset',state))] for state in states]","metadata":{"id":"zX30hkgVt-CJ","execution":{"iopub.status.busy":"2022-11-07T10:25:18.994406Z","iopub.execute_input":"2022-11-07T10:25:18.995242Z","iopub.status.idle":"2022-11-07T10:25:20.391706Z","shell.execute_reply.started":"2022-11-07T10:25:18.995202Z","shell.execute_reply":"2022-11-07T10:25:20.390722Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"images_paths=[]\nfor i in range(3):\n  for j in range(len(image_files[i])):\n    current=image_files[i]\n    images_paths.append(current[j])","metadata":{"id":"B2V49ZmnvUk2","execution":{"iopub.status.busy":"2022-11-07T10:25:20.394506Z","iopub.execute_input":"2022-11-07T10:25:20.394882Z","iopub.status.idle":"2022-11-07T10:25:20.403962Z","shell.execute_reply.started":"2022-11-07T10:25:20.394846Z","shell.execute_reply":"2022-11-07T10:25:20.402796Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(nn.Module):\n  def __init__(self,image_paths,To_tensor):\n    super(CustomDataset,self).__init__()\n    self.image_paths=image_paths\n    self.To_tensor=To_tensor\n\n  def __len__(self):\n    return len(self.image_paths)\n\n  def __getitem__(self,x):\n    image=cv2.imread(self.image_paths[x])\n    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    imgae=np.asarray(image,dtype=np.uint8)\n\n    image=Image.fromarray(image.astype(np.uint8))\n    image=self.To_tensor(image)\n\n    return image.clone()","metadata":{"id":"oAtsQpPTtoj0","execution":{"iopub.status.busy":"2022-11-07T10:25:20.405559Z","iopub.execute_input":"2022-11-07T10:25:20.406704Z","iopub.status.idle":"2022-11-07T10:25:20.414751Z","shell.execute_reply.started":"2022-11-07T10:25:20.406669Z","shell.execute_reply":"2022-11-07T10:25:20.413854Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Transformation\nOur model input image size 64. So we resize image and check training image.","metadata":{"id":"FTIZBBxYcXtN"}},{"cell_type":"code","source":"To_tensor=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((params['image_size'],params['image_size'])),\n    transforms.Normalize(        \n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]),\n])\n\ndataset=CustomDataset(images_paths,To_tensor)\n\ndataloader=DataLoader(dataset,\n                      batch_size=params['batch_size'],\n                      shuffle=True,\n                      num_workers=params['num_workers'],\n                      pin_memory=True\n)\n\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"9zNq7DWLwFN2","outputId":"d02a831c-654f-42ee-af1c-fcb741b1ed44","execution":{"iopub.status.busy":"2022-11-07T10:25:20.417855Z","iopub.execute_input":"2022-11-07T10:25:20.418109Z","iopub.status.idle":"2022-11-07T10:25:28.742044Z","shell.execute_reply.started":"2022-11-07T10:25:20.418086Z","shell.execute_reply":"2022-11-07T10:25:28.740556Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f327a174250>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Model\n\n\n**G_conv** : Convolution in Generator model.<br>\n**D_conv** : Convolution in Discriminator model. <br>\n**DHead** : DHead apply sigmoid from Discriminator output. <br>\n**QHead** : QHead make mu(population mean), var(variance), disc_logits <br>\n**Generator** : Generate image <br>\n**Discriminator** : Discriminator image","metadata":{"id":"9FJz_jrlctpc"}},{"cell_type":"code","source":"\nclass G_conv(nn.Module):\n    def __init__(self,in_channels,out_channels,*args):\n        super(G_conv,self).__init__()\n        self.conv=nn.Sequential(\n            nn.ConvTranspose2d( in_channels,out_channels,*args, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n    def forward(self,x):\n        return self.conv(x)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator,self).__init__()\n        self.main=nn.Sequential(\n            G_conv(params['nz'],params['ngf']*8,4,1,0),\n            G_conv(params['ngf']*8,params['ngf']*4,4,2,1),\n            G_conv(params['ngf']*4,params['ngf']*2,4,2,1),\n            G_conv(params['ngf']*2,params['ngf'],4,2,1),\n\n            nn.ConvTranspose2d(params['ngf'],params['nc'],4,2,1,bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self,x):\n        return self.main(x)\n\nclass D_conv(nn.Module):\n    def __init__(self,in_channels,out_channels,*args):\n        super(D_conv,self).__init__()\n        self.conv=nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, *args, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self,x):\n        return self.conv(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator,self).__init__()\n        self.main=nn.Sequential(\n            nn.Conv2d(params['nc'],params['ndf'],4,2,1,bias=False),\n            nn.LeakyReLU(0.2,inplace=True),\n            D_conv(params['ndf'],params['ndf']*2,4,2,1),\n            D_conv(params['ndf']*2,params['ndf']*4,4,2,1),\n            D_conv(params['ndf']*4,params['ndf']*8,4,2,1),\n            nn.Conv2d(params['ndf']*8,params['ndf']*8,4,1,0,bias=False),\n        )\n\n    def forward(self,x):\n        return self.main(x)\n\nclass DHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv=nn.Conv2d(params['ndf']*8,1,1,bias=False)\n    \n    def forward(self,x):\n        x=torch.sigmoid(self.conv(x))\n        return x\nclass QHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1=nn.Conv2d(params['ndf']*8,128,1,bias=False)\n        self.bn1=nn.BatchNorm2d(128)\n        self.leakyrelu=nn.LeakyReLU(0.2,inplace=True)\n        self.conv_disc=nn.Conv2d(128,10,1)\n        self.conv_mu=nn.Conv2d(128,2,1)\n        self.conv_var=nn.Conv2d(128,2,1)\n\n    def forward(self,x):\n        x=self.leakyrelu(self.bn1(self.conv1(x)))\n        \n        disc_logits=self.conv_disc(x).squeeze()\n\n        mu=self.conv_mu(x).squeeze()\n        var=torch.exp(self.conv_var(x).squeeze())\n\n        return disc_logits,mu,var","metadata":{"id":"67PfHwqYw0RV","execution":{"iopub.status.busy":"2022-11-07T10:25:28.744504Z","iopub.execute_input":"2022-11-07T10:25:28.745188Z","iopub.status.idle":"2022-11-07T10:25:28.770507Z","shell.execute_reply.started":"2022-11-07T10:25:28.745150Z","shell.execute_reply":"2022-11-07T10:25:28.769805Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    if isinstance(m,nn.Conv2d):\n        nn.init.normal_(m.weight.data,0.0,0.02)\n    elif isinstance(m,nn.BatchNorm2d):\n        nn.init.normal_(m.weight.data,1.0,0.02)\n        nn.init.constant_(m.bias.data,0)\n\nclass NormalNLLLoss:\n    def __call__(self,x,mu,var):\n        logli=-0.5*(var.mul(2*np.pi)+1e-6).log()-(x-mu).pow(2).div(var.mul(2.0)+1e-6)\n        nll=-(logli.sum(1).mean())\n\n        return nll\n\ndef noise_sample(n_dis_c,dis_c_dim,n_con_c,n_z,batch_size,device):\n    z=torch.randn(batch_size,n_z,1,1,device=device)\n\n    idx=np.zeros((n_dis_c,batch_size))\n    if n_dis_c !=0:\n        dis_c=torch.zeros(batch_size,n_dis_c,dis_c_dim,device=device)\n\n        for i in range(n_dis_c):\n            idx[i]=np.random.randint(dis_c_dim,size=batch_size)\n            dis_c[torch.arange(0,batch_size),i,idx[i]]=1.0\n        \n        dis_c=dis_c.view(batch_size,-1,1,1)\n\n    if n_con_c!=0:\n        con_c=torch.rand(batch_size,n_con_c,1,1,device=device)*2-1\n\n    noise=z\n\n    if n_dis_c!=0:\n        noise=torch.cat((z,dis_c),dim=1)\n    if n_con_c!=0:\n        noise=torch.cat((noise,con_c),dim=1)\n    return noise,idx","metadata":{"id":"Jrsukzq1zMeU","execution":{"iopub.status.busy":"2022-11-07T10:25:28.772559Z","iopub.execute_input":"2022-11-07T10:25:28.773339Z","iopub.status.idle":"2022-11-07T10:25:28.788424Z","shell.execute_reply.started":"2022-11-07T10:25:28.773298Z","shell.execute_reply":"2022-11-07T10:25:28.787509Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"id":"pPaTpz83gipH"}},{"cell_type":"code","source":"params['num_z']=62\nparams['num_dis_c']=1\nparams['dis_c_dim']=10\nparams['num_con_c']=2\n\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n\nnetG=Generator().to(device)\ndiscriminator=Discriminator().to(device)\nnetD=DHead().to(device)\nnetQ=QHead().to(device)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\nnetQ.apply(weights_init)\n\ncriterionD=nn.BCELoss()\ncriterionQ_dis=nn.CrossEntropyLoss()\ncriterionQ_con=NormalNLLLoss()\n\nfixed_noise=torch.randn(64,params['nz'],1,1,device=device)\n\n\noptimD=optim.Adam([{'params': discriminator.parameters()}, {'params': netD.parameters()}],lr=params['lr'],betas=(params['beta1'],0.999))\noptimG=optim.Adam([{'params': netG.parameters()}, {'params': netQ.parameters()}],lr=params['lr'],betas=(params['beta1'],0.999))\n\n#fixed noise\nz = torch.randn(100, params['num_z'], 1, 1, device=device)\nfixed_noise = z\nif(params['num_dis_c'] != 0):\n    idx = np.arange(params['dis_c_dim']).repeat(10)\n    dis_c = torch.zeros(100, params['num_dis_c'], params['dis_c_dim'], device=device)\n    for i in range(params['num_dis_c']):\n        dis_c[torch.arange(0, 100), i, idx] = 1.0\n\n    dis_c = dis_c.view(100, -1, 1, 1)\n\n    fixed_noise = torch.cat((fixed_noise, dis_c), dim=1)\n\nif(params['num_con_c'] != 0):\n    con_c = torch.rand(100, params['num_con_c'], 1, 1, device=device) * 2 - 1\n    fixed_noise = torch.cat((fixed_noise, con_c), dim=1)\n\nreal_label=1.\nfake_label=0.\n\nimg_list=[]\nG_losses=[]\nD_losses=[]\n\niters=0\nprint('start')\nfor epoch in range(params['epochs']):\n    epoch_start_time = time.time()\n    for i, data in enumerate(dataloader,0):\n        b_size=data.size(0)\n        real_data=data.to(device)\n\n        optimD.zero_grad()\n        label=torch.full((b_size,),real_label,device=device)\n        output1=discriminator(real_data)\n        probs_real=netD(output1).view(-1)\n        loss_real=criterionD(probs_real,label)\n        loss_real.backward()\n\n        label.fill_(fake_label)\n        noise,idx=noise_sample(params['num_dis_c'],params['dis_c_dim'],params['num_con_c'],params['num_z'],b_size,device)\n        fake_data=netG(noise)\n        output2=discriminator(fake_data.detach())\n        probs_fake=netD(output2).view(-1)\n        loss_fake=criterionD(probs_fake,label)\n\n        loss_fake.backward()\n\n        D_loss=loss_real+loss_fake\n        optimD.step()\n        optimG.zero_grad()\n\n        output=discriminator(fake_data)\n        label.fill_(real_label)\n        probs_fake=netD(output).view(-1)\n        gen_loss=criterionD(probs_fake,label)\n\n        q_logits,q_mu,q_var=netQ(output)\n        target=torch.LongTensor(idx).to(device)\n\n        dis_loss=0\n        for j in range(params['num_dis_c']):\n            dis_loss+=criterionQ_dis(q_logits[:,j*10:j*10+10],target[j])\n\n        con_loss=0\n        if params['num_con_c']!=0:\n            con_loss=criterionQ_con(noise[:,params['num_z']+params['num_dis_c']*params['dis_c_dim']:].view(-1,params['num_con_c']),q_mu,q_var)*0.1\n        G_loss=gen_loss+dis_loss+con_loss\n        G_loss.backward()\n        optimG.step()\n        if i != 0 and i%100 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n                  % (epoch+1, params['epochs'], i, len(dataloader), \n                    D_loss.item(), G_loss.item()))\n\n        # Save the losses for plotting.\n        G_losses.append(G_loss.item())\n        D_losses.append(D_loss.item())\n\n        iters += 1\n    epoch_time = time.time() - epoch_start_time\n    print(\"Time taken for Epoch %d: %.2fs\" %(epoch + 1, epoch_time))\n    \n    # Generate image after each epoch to check performance of the generator. Used for creating animated gif later.\n    with torch.no_grad():\n        gen_data = netG(fixed_noise).detach().cpu()\n    img_list.append(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True))\n\n    # Generate image to check performance of generator.\n    if((epoch+1) == 1 or (epoch+1) == params['epochs']/2):\n        with torch.no_grad():\n            gen_data = netG(fixed_noise).detach().cpu()\n        plt.figure(figsize=(10, 10))\n        plt.axis(\"off\")\n        plt.imshow(np.transpose(vutils.make_grid(gen_data, nrow=10, padding=2, normalize=True), (1,2,0)))\n        plt.savefig(\"Epoch_%d {}\".format('MNIST') %(epoch+1))\n        plt.close('all')\n\n","metadata":{"id":"6NbteyK9zUag","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f66bdfb-3839-4b11-d948-fbe3de08da84","execution":{"iopub.status.busy":"2022-11-07T10:25:28.790457Z","iopub.execute_input":"2022-11-07T10:25:28.791150Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"start\nTime taken for Epoch 1: 64.01s\nTime taken for Epoch 2: 24.91s\nTime taken for Epoch 3: 24.99s\nTime taken for Epoch 4: 24.56s\nTime taken for Epoch 5: 25.27s\nTime taken for Epoch 6: 25.23s\nTime taken for Epoch 7: 25.52s\nTime taken for Epoch 8: 24.64s\nTime taken for Epoch 9: 24.87s\nTime taken for Epoch 10: 25.71s\nTime taken for Epoch 11: 25.23s\nTime taken for Epoch 12: 25.60s\nTime taken for Epoch 13: 24.80s\nTime taken for Epoch 14: 26.77s\nTime taken for Epoch 15: 25.14s\nTime taken for Epoch 16: 25.56s\nTime taken for Epoch 17: 25.72s\nTime taken for Epoch 18: 25.21s\nTime taken for Epoch 19: 25.40s\nTime taken for Epoch 20: 25.65s\nTime taken for Epoch 21: 25.58s\nTime taken for Epoch 22: 25.47s\nTime taken for Epoch 23: 24.79s\nTime taken for Epoch 24: 26.72s\nTime taken for Epoch 25: 24.94s\nTime taken for Epoch 26: 25.43s\nTime taken for Epoch 27: 24.85s\nTime taken for Epoch 28: 24.98s\nTime taken for Epoch 29: 25.33s\nTime taken for Epoch 30: 25.26s\nTime taken for Epoch 31: 25.80s\nTime taken for Epoch 32: 25.59s\nTime taken for Epoch 33: 25.30s\nTime taken for Epoch 34: 25.45s\nTime taken for Epoch 35: 25.36s\nTime taken for Epoch 36: 25.46s\nTime taken for Epoch 37: 25.49s\nTime taken for Epoch 38: 25.10s\nTime taken for Epoch 39: 25.10s\nTime taken for Epoch 40: 25.99s\nTime taken for Epoch 41: 25.21s\nTime taken for Epoch 42: 25.27s\nTime taken for Epoch 43: 25.03s\nTime taken for Epoch 44: 25.14s\nTime taken for Epoch 45: 24.92s\nTime taken for Epoch 46: 24.62s\nTime taken for Epoch 47: 24.76s\nTime taken for Epoch 48: 24.46s\nTime taken for Epoch 49: 24.66s\nTime taken for Epoch 50: 24.56s\nTime taken for Epoch 51: 24.58s\nTime taken for Epoch 52: 24.13s\nTime taken for Epoch 53: 24.79s\nTime taken for Epoch 54: 24.80s\nTime taken for Epoch 55: 24.56s\nTime taken for Epoch 56: 24.08s\nTime taken for Epoch 57: 24.39s\nTime taken for Epoch 58: 25.48s\nTime taken for Epoch 59: 24.56s\nTime taken for Epoch 60: 24.97s\nTime taken for Epoch 61: 24.46s\nTime taken for Epoch 62: 25.09s\nTime taken for Epoch 63: 24.67s\nTime taken for Epoch 64: 24.74s\nTime taken for Epoch 65: 24.19s\nTime taken for Epoch 66: 24.60s\nTime taken for Epoch 67: 24.84s\nTime taken for Epoch 68: 25.02s\nTime taken for Epoch 69: 25.68s\nTime taken for Epoch 70: 25.12s\nTime taken for Epoch 71: 26.12s\nTime taken for Epoch 72: 27.05s\nTime taken for Epoch 73: 26.82s\nTime taken for Epoch 74: 26.84s\nTime taken for Epoch 75: 25.91s\nTime taken for Epoch 76: 26.13s\nTime taken for Epoch 77: 26.33s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Result\n\n|Generation GIF|Generation Image|\n|------|--------|\n|<img src='https://raw.githubusercontent.com/cjfghk5697/Code_Share/main/GAN/Shoe%20vs%20Sandal%20vs%20Boot%20Image%20Dataset/results/results.gif' alt=\"results.gif\"/>|<img src='https://raw.githubusercontent.com/cjfghk5697/Code_Share/main/GAN/Shoe%20vs%20Sandal%20vs%20Boot%20Image%20Dataset/results/7.png' alt=\"results.gif\"/>|\n\n","metadata":{"id":"NFcObjtqgj8n"}},{"cell_type":"markdown","source":"**Training Loss Curve**","metadata":{"id":"MDZoqJIZmphy"}},{"cell_type":"code","source":"plt.figure()\nplt.plot(G_losses, color='blue', marker=',', label='G')\nplt.plot(D_losses, color='orange', marker=',', label='D')\nplt.title('Generation and Discriminator loss')\nplt.legend()\nplt.show()","metadata":{"id":"2oVycM7UmNRS","trusted":true},"execution_count":null,"outputs":[]}]}